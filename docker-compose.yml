version: '3.8'

# Network configuration
networks:
  monetic-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
  ctf-target-net:
    driver: bridge
    ipam:
      config:
        - subnet: 10.10.10.0/24
    driver_opts:
      com.docker.network.bridge.name: ctf0

# Persistent volumes
volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  pgadmin-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  elasticsearch-data:
    driver: local

services:
  # ============================================
  # DATABASE SERVICES
  # ============================================
  
  postgres:
    image: postgres:14-alpine
    container_name: pmp-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: ${POSTGRES_INITDB_ARGS}
      PGDATA: ${PGDATA}
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-databases.sh:/docker-entrypoint-initdb.d/01-init-databases.sh:ro
      - ./scripts/seed-pedagogical-data.sql:/docker-entrypoint-initdb.d/02-seed-data.sql:ro
    ports:
      - "5435:5432"
    networks:
      monetic-network: {}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7-alpine
    container_name: pmp-redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    networks:
      monetic-network: {}
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pmp-pgadmin
    profiles: ["dev"]
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
      PGADMIN_LISTEN_PORT: ${PGADMIN_LISTEN_PORT}
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    ports:
      - "5050:80"
    networks:
      - monetic-network
    depends_on:
      - postgres
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  docker-socket-proxy:
    image: tecnativa/docker-socket-proxy:latest
    container_name: pmp-docker-socket-proxy
    restart: unless-stopped
    environment:
      - CONTAINERS=1
      - IMAGES=1
      - NETWORKS=1
      - INFO=1
      - VERSION=1
      - POST=1
      - DELETE=1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - monetic-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  lab-orchestrator:
    build:
      context: .
      dockerfile: backend/lab-orchestrator/Dockerfile
    container_name: pmp-lab-orchestrator
    restart: unless-stopped
    environment:
      - PORT=8098
      - LAB_ORCHESTRATOR_SECRET=${LAB_ORCHESTRATOR_SECRET}
      - DOCKER_PROXY_URL=http://docker-socket-proxy:2375
      - LAB_ENABLE_DOCKER_ACTIONS=${LAB_ENABLE_DOCKER_ACTIONS:-true}
      - LAB_ACCESS_PROXY_BASE_PATH=${LAB_ACCESS_PROXY_BASE_PATH:-/lab}
      - LAB_ATTACKBOX_CONTAINER_NAME=${LAB_ATTACKBOX_CONTAINER_NAME:-pmp-ctf-attackbox}
      - LAB_ATTACKBOX_CONTAINER_IMAGE=${LAB_ATTACKBOX_CONTAINER_IMAGE:-pmp-ctf-attackbox}
    ports:
      - "8098:8098"
    networks:
      - monetic-network
    depends_on:
      - docker-socket-proxy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8098/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  lab-access-proxy:
    build:
      context: .
      dockerfile: backend/lab-access-proxy/Dockerfile
    container_name: pmp-lab-access-proxy
    restart: unless-stopped
    environment:
      - PORT=8099
      - API_GATEWAY_URL=http://api-gateway:8000
      - LAB_ACCESS_PROXY_BASE_PATH=${LAB_ACCESS_PROXY_BASE_PATH:-/lab}
      - LAB_INTERNAL_PROXY_SECRET=${LAB_INTERNAL_PROXY_SECRET}
      - INTERNAL_HSM_SECRET=${INTERNAL_HSM_SECRET}
      - JWT_SECRET=${JWT_SECRET}
      - LAB_PROXY_CACHE_TTL_MS=${LAB_PROXY_CACHE_TTL_MS:-15000}
    ports:
      - "8099:8099"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8099/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # FRONTEND SERVICES
  # ============================================

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    container_name: pmp-elasticsearch
    profiles: ["monitoring"]
    environment:
      - node.name=elasticsearch
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - monetic-network
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q 'status.*green\\|status.*yellow'"]
      interval: 30s
      timeout: 10s
      retries: 3

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.9
    container_name: pmp-kibana
    profiles: ["monitoring"]
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
    networks:
      - monetic-network
    depends_on:
      - elasticsearch

  portal:
    build:
      context: ./frontend
      dockerfile: portal/Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        - INTERNAL_API_URL=http://api-gateway:8000
        - INTERNAL_LAB_PROXY_URL=http://lab-access-proxy:8099
        - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
        - NEXT_PUBLIC_TPE_URL=${NEXT_PUBLIC_TPE_URL:-http://localhost:3001}
        - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
        - NEXT_PUBLIC_HSM_URL=${NEXT_PUBLIC_HSM_URL:-http://localhost:3006}
        - NEXT_PUBLIC_MONITORING_URL=${NEXT_PUBLIC_MONITORING_URL:-http://localhost:3082}
        - NEXT_PUBLIC_CTF_ATTACKBOX_URL=${NEXT_PUBLIC_CTF_ATTACKBOX_URL:-http://localhost:7681}
    container_name: pmp-portal
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=3000
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - INTERNAL_API_URL=http://api-gateway:8000
      - INTERNAL_LAB_PROXY_URL=http://lab-access-proxy:8099
      - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
      - NEXT_PUBLIC_TPE_URL=${NEXT_PUBLIC_TPE_URL:-http://localhost:3001}
      - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
      - NEXT_PUBLIC_HSM_URL=${NEXT_PUBLIC_HSM_URL:-http://localhost:3006}
      - NEXT_PUBLIC_MONITORING_URL=${NEXT_PUBLIC_MONITORING_URL:-http://localhost:3082}
      - NEXT_PUBLIC_CTF_ATTACKBOX_URL=${NEXT_PUBLIC_CTF_ATTACKBOX_URL:-http://localhost:7681}
    ports:
      - "3000:3000"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
      - lab-access-proxy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M

  ctf-attackbox:
    build:
      context: .
      dockerfile: docker/ctf-attackbox/Dockerfile
    container_name: pmp-ctf-attackbox
    profiles: ["ctf"]
    restart: unless-stopped
    environment:
      - WETTY_HOST=0.0.0.0
      - WETTY_PORT=7681
      - PMP_GATEWAY_URL=http://api-gateway:8000
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "7681:7681"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.20
    dns:
      - 127.0.0.1
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - NET_BROADCAST
    depends_on:
      - api-gateway
      - hsm-simulator
      - sim-network-switch
      - sim-issuer-service
      - sim-pos-service
      - acs-simulator
      - sim-fraud-detection
    healthcheck:
      test: ["CMD-SHELL", "code=$$(curl -s -o /dev/null -w '%{http_code}' http://127.0.0.1:7681/ || true); [ \"$$code\" = \"200\" ] || [ \"$$code\" = \"401\" ]"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  client-interface:
    build:
      context: ./frontend
      dockerfile: tpe-web/Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
        - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
    container_name: pmp-client-interface
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
      - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
    ports:
      - "3001:3000"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    cap_drop:
      - ALL


  sim-monitoring-service:
    build:
      context: ./backend/monitoring-service
      dockerfile: Dockerfile
    container_name: pmp-monitoring-service
    profiles: ["monitoring"]
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=3005
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
    ports:
      - "3005:3005"
    networks:
      - monetic-network
    depends_on:
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3005/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=3005"
      - "prometheus.path=/metrics"

  user-cards-web:
    build:
      context: ./frontend
      dockerfile: user-cards-web/Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
        - NEXT_PUBLIC_TPE_URL=${NEXT_PUBLIC_TPE_URL:-http://localhost:3001}
        - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
    container_name: pmp-user-cards-web
    restart: unless-stopped
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=3000
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
      - NEXT_PUBLIC_TPE_URL=${NEXT_PUBLIC_TPE_URL:-http://localhost:3001}
      - NEXT_PUBLIC_USER_CARDS_URL=${NEXT_PUBLIC_USER_CARDS_URL:-http://localhost:3004}
    ports:
      - "3004:3000"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    cap_drop:
      - ALL

  hsm-web:
    build:
      context: ./frontend
      dockerfile: hsm-web/Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
    container_name: pmp-hsm-web
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=3000
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NEXT_PUBLIC_PORTAL_URL=${NEXT_PUBLIC_PORTAL_URL:-http://localhost:3000}
    ports:
      - "3006:3000"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  threeds-challenge-ui:
    build:
      context: ./frontend
      dockerfile: 3ds-challenge-ui/Dockerfile
    container_name: pmp-3ds-challenge-ui
    restart: unless-stopped
    ports:
      - "3088:3088"
    networks:
      - monetic-network
    depends_on:
      - api-gateway
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:3088"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  monitoring-dashboard:
    build:
      context: ./frontend
      dockerfile: monitoring-dashboard/Dockerfile
    container_name: pmp-monitoring-dashboard
    profiles: ["monitoring"]
    restart: unless-stopped
    ports:
      - "3082:3000"
    networks:
      - monetic-network
    depends_on:
      - sim-monitoring-service
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # API GATEWAY
  # ============================================

  api-gateway:
    build:
      context: ./backend/api-gateway
      dockerfile: Dockerfile
    container_name: pmp-api-gateway
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8000
      - LOG_LEVEL=${LOG_LEVEL}
      - JWT_SECRET=${JWT_SECRET}
      - INTERNAL_HSM_SECRET=${INTERNAL_HSM_SECRET}
      - RATE_LIMIT_WINDOW_MS=${RATE_LIMIT_WINDOW_MS}
      - RATE_LIMIT_MAX_REQUESTS=${RATE_LIMIT_MAX_REQUESTS}
      - LAB_MAX_ACTIVE_SESSIONS=${LAB_MAX_ACTIVE_SESSIONS:-20}
      - LAB_DEFAULT_TTL_MINUTES=${LAB_DEFAULT_TTL_MINUTES:-120}
      - LAB_EXTENSION_MINUTES=${LAB_EXTENSION_MINUTES:-60}
      - LAB_MAX_EXTENSIONS=${LAB_MAX_EXTENSIONS:-1}
      - LAB_MAINTENANCE_INTERVAL_MS=${LAB_MAINTENANCE_INTERVAL_MS:-30000}
      - LAB_RECONCILE_INTERVAL_MS=${LAB_RECONCILE_INTERVAL_MS:-120000}
      - LAB_PROVISIONING_TIMEOUT_MINUTES=${LAB_PROVISIONING_TIMEOUT_MINUTES:-10}
      - LAB_NETWORK_BASE_CIDR=${LAB_NETWORK_BASE_CIDR:-10.60.0.0/16}
      - LAB_NETWORK_SUBNET_PREFIX=${LAB_NETWORK_SUBNET_PREFIX:-28}
      - LAB_ORCHESTRATOR_URL=http://lab-orchestrator:8098
      - LAB_ORCHESTRATOR_SECRET=${LAB_ORCHESTRATOR_SECRET}
      - LAB_ACCESS_PROXY_BASE_PATH=${LAB_ACCESS_PROXY_BASE_PATH:-/lab}
      - LAB_INTERNAL_PROXY_SECRET=${LAB_INTERNAL_PROXY_SECRET}
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - CARD_SERVICE_URL=http://sim-card-service:8001
      - POS_SERVICE_URL=http://sim-pos-service:8002
      - ACQUIRER_SERVICE_URL=http://sim-acquirer-service:8003
      - SIM_CARD_SERVICE_URL=http://sim-card-service:8001
      - SIM_POS_SERVICE_URL=http://sim-pos-service:8002
      - SIM_ACQUIRER_SERVICE_URL=http://sim-acquirer-service:8003
      - SIM_NETWORK_SWITCH_URL=http://sim-network-switch:8004
      - SIM_ISSUER_SERVICE_URL=http://sim-issuer-service:8005
      - SIM_AUTH_ENGINE_URL=http://sim-auth-engine:8006
      - SIM_FRAUD_DETECTION_URL=http://sim-fraud-detection:8007
      - CRYPTO_SERVICE_URL=http://crypto-service:8010
      - HSM_SIMULATOR_URL=http://hsm-simulator:8011
      - KEY_MANAGEMENT_URL=http://key-management:8012
      - ACS_SIMULATOR_URL=http://acs-simulator:8013
      - TOKENIZATION_URL=http://tokenization-service:8014
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
      - CORS_ORIGIN=${CORS_ORIGIN}
    ports:
      - "8000:8000"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.10
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sim-card-service:
        condition: service_started
      sim-pos-service:
        condition: service_started
      sim-acquirer-service:
        condition: service_started
      lab-orchestrator:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8000"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  # ============================================
  # CORE MICROSERVICES
  # ============================================

  sim-card-service:
    build:
      context: ./backend/sim-card-service
      dockerfile: Dockerfile
    container_name: pmp-sim-card-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8001
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - CRYPTO_SERVICE_URL=http://crypto-service:8010
    ports:
      - "8001:8001"
    networks:
      - monetic-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crypto-service:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8001/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8001"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-pos-service:
    build:
      context: ./backend/sim-pos-service
      dockerfile: Dockerfile
    container_name: pmp-sim-pos-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8002
      - CRYPTO_SERVICE_URL=http://crypto-service:8010
      - ACQUIRER_SERVICE_URL=http://sim-acquirer-service:8003
    ports:
      - "8002:8002"
    networks:
      - monetic-network
    depends_on:
      - crypto-service
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8002/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8002"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-acquirer-service:
    build:
      context: ./backend/sim-acquirer-service
      dockerfile: Dockerfile
    container_name: pmp-sim-acquirer-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8003
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - NETWORK_SWITCH_URL=http://sim-network-switch:8004
    ports:
      - "8003:8003"
    networks:
      - monetic-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8003/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8003"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-network-switch:
    build:
      context: ./backend/sim-network-switch
      dockerfile: Dockerfile
    container_name: pmp-sim-network-switch
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8004
      - ISSUER_SERVICE_URL=http://sim-issuer-service:8005
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8004:8004"
      - "8583:8583"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.12
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8004/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8004"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-issuer-service:
    build:
      context: ./backend/sim-issuer-service
      dockerfile: Dockerfile
    container_name: pmp-sim-issuer-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8005
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AUTH_ENGINE_URL=http://sim-auth-engine:8006
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8005:8005"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.14
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8005/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8005"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-auth-engine:
    build:
      context: ./backend/sim-auth-engine
      dockerfile: Dockerfile
    container_name: pmp-sim-auth-engine
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8006
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - CRYPTO_SERVICE_URL=http://crypto-service:8010
      - FRAUD_DETECTION_URL=http://sim-fraud-detection:8007
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8006:8006"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.16
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      crypto-service:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8006/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8006"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  sim-fraud-detection:
    build:
      context: ./backend/sim-fraud-detection
      dockerfile: Dockerfile
    container_name: pmp-sim-fraud-detection
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8007
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - FRAUD_THRESHOLD_LOW=${FRAUD_THRESHOLD_LOW}
      - FRAUD_THRESHOLD_MEDIUM=${FRAUD_THRESHOLD_MEDIUM}
      - FRAUD_THRESHOLD_HIGH=${FRAUD_THRESHOLD_HIGH}
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8007:8007"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.15
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:8007/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8007"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  # ============================================
  # SECURITY SERVICES
  # ============================================

  crypto-service:
    build:
      context: ./backend/crypto-service
      dockerfile: Dockerfile
    container_name: pmp-crypto-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8010
      - HSM_SIMULATOR_URL=http://hsm-simulator:8011
      - KEY_MANAGEMENT_URL=http://key-management:8012
      - PIN_ENCRYPTION_ALGORITHM=${PIN_ENCRYPTION_ALGORITHM}
      - MAC_ALGORITHM=${MAC_ALGORITHM}
    ports:
      - "8010:8010"
    networks:
      - monetic-network
    depends_on:
      - hsm-simulator
      - key-management
    volumes:
      - ./keys:/app/keys:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8010/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8010"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  hsm-simulator:
    build:
      context: ./backend/hsm-simulator
      dockerfile: Dockerfile
    container_name: pmp-hsm-simulator
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8011
      - LMK_KEY_FILE=${LMK_KEY_FILE}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - INTERNAL_HSM_SECRET=${INTERNAL_HSM_SECRET}
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8011:8011"
      - "5959:5959"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.11
    volumes:
      - ./keys:/app/keys:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8011/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8011"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  key-management:
    build:
      context: ./backend/key-management
      dockerfile: Dockerfile
    container_name: pmp-key-management
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8012
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - KEY_ROTATION_INTERVAL_DAYS=${KEY_ROTATION_INTERVAL_DAYS}
      - MASTER_KEY_FILE=${MASTER_KEY_FILE}
    ports:
      - "8012:8012"
    networks:
      - monetic-network
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./keys:/app/keys
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8012/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8012"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  # ============================================
  # ADVANCED EXTENSIONS
  # ============================================

  acs-simulator:
    build:
      context: ./backend/acs-simulator
      dockerfile: Dockerfile
    container_name: pmp-acs-simulator
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8013
      - CTF_FLAG_SECRET=${CTF_FLAG_SECRET:-pmp_ctf_default_secret_change_in_prod}
    ports:
      - "8013:8013"
    networks:
      monetic-network: {}
      ctf-target-net:
        ipv4_address: 10.10.10.13
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8013/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8013"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  tokenization-service:
    build:
      context: ./backend/tokenization-service
      dockerfile: Dockerfile
    container_name: pmp-tokenization-service
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8014
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
    ports:
      - "8014:8014"
    networks:
      - monetic-network
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8014/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8014"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL

  directory-server:
    build:
      context: ./backend/directory-server
      dockerfile: Dockerfile
    container_name: pmp-directory-server
    restart: unless-stopped
    user: "node:node"
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=8015
      - ACS_SIMULATOR_URL=http://acs-simulator:8013
    ports:
      - "8015:8015"
    networks:
      - monetic-network
    depends_on:
      - acs-simulator
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8015/health"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "prometheus.scrape=true"
      - "prometheus.port=8015"
      - "prometheus.path=/metrics"
    cap_drop:
      - ALL




  # ============================================
  # NGINX REVERSE PROXY
  # ============================================

  nginx:
    image: nginx:alpine
    container_name: pmp-nginx
    restart: unless-stopped
    command: >
      /bin/sh -c
      "while :; do sleep 6h & wait $$!; nginx -s reload; done &
      nginx -g 'daemon off;'"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./certbot/www:/var/www/certbot:ro
      - ./nginx/logs:/var/log/nginx
    networks:
      monetic-network:
        ipv4_address: 172.20.0.100
    depends_on:
      - api-gateway
      - client-interface
      - lab-access-proxy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  certbot-renew:
    image: certbot/certbot:latest
    container_name: pmp-certbot-renew
    restart: unless-stopped
    entrypoint: /bin/sh
    command: -c "trap exit TERM; while :; do certbot renew --webroot -w /var/www/certbot --quiet --deploy-hook 'sh /opt/ssl/certbot-deploy-hook.sh'; sleep 12h; done"
    volumes:
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
      - ./nginx/ssl:/etc/nginx/ssl
      - ./scripts/ssl:/opt/ssl:ro
    depends_on:
      - nginx

  # ============================================
  # MONITORING SERVICES
  # ============================================

  prometheus:
    image: prom/prometheus:latest
    container_name: pmp-prometheus
    profiles: ["monitoring"]
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - monetic-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:latest
    container_name: pmp-grafana
    profiles: ["monitoring"]
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=${GF_INSTALL_PLUGINS}
      - GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana-provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana-provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana-dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3002:3000"
    networks:
      - monetic-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
